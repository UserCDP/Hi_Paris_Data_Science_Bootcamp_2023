{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Python libraries\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZyhKqCkh5D8S",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None) #Show all columns\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can now import the dataset you worked on in Data Cleaning and Data visualization with pandas.** <br>\n",
    "\n",
    "First, here is how you save a dataframe to csv from your Data cleaning/data viz notebooks.\n",
    "\n",
    "```\n",
    "# Create a dataframe with pd.Dataframe()\n",
    "df = pd.DataFrame({'name': ['Raphael', 'Donatello'],\n",
    "                   'mask': ['red', 'purple'],\n",
    "                   'weapon': ['sai', 'bo staff']})\n",
    "                   \n",
    "# Save the dataframe to a csv format                   \n",
    "df.to_csv(\"new_dataset.csv\", index=False)\n",
    "```\n",
    "\n",
    "Then, use `dataset = pd.read_csv(\"new_dataset.csv\")` in this notebook to load the `\"new_dataset.csv\"` csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you don't want to use the dataframe form data cleaning and data viz, then run this code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path=r'~/hfactory_magic_folders/course/Dataset/dataset_train.csv'\n",
    "\n",
    "# Import the csv file\n",
    "dataset = pd.read_csv(path,encoding='latin-1',sep=';')\n",
    "\n",
    "# Clean the dataframe \n",
    "to_drop=['Customer Email','Customer Fname','Customer Lname','Customer Password','Customer Street','Order Zipcode','Product Description']\n",
    "dataset=dataset.drop(to_drop,axis=1)\n",
    "dataset=dataset.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import sklearn models, preprocessing tools, metrics\n",
    "Here we've provided a list of scikit-learn models you can use. \n",
    "\n",
    "**Note**: <br>\n",
    "You don't have to use these specific models to train your data. You can try other scikit-learn models. <br>\n",
    "Part of a data scientist's job is to try out and discover new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZS8yjKdl5dGt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Classification models\n",
    "from sklearn.linear_model import LogisticRegression,LinearRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Preprocessing tools\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder,StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Performance metrics\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, classification_report, explained_variance_score\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "# Improve your model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Target value**: <br>\n",
    "You have two options for the target variable: \n",
    "- `Late_delivery_risk` (binary classification) \n",
    "- `Delivery_Status` (multi-class classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If you pick `Late_delivery_risk`, `Delivery_status` should be deleted from the dataset (and vice-versa). <br>\n",
    "If you need help doing this, the next cell will do it for you.\n",
    "- Keep `binary_classification=True` if you pick binary classification.\n",
    "- Change it to `binary_classification=False` if you pick multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose your figther\n",
    "binary_classification=False\n",
    "\n",
    "if binary_classification:\n",
    "    dataset=dataset.drop(columns=['Delivery Status'])\n",
    "    name_label=['Late_delivery_risk']\n",
    "else:\n",
    "    # dataset=dataset.drop('Late_delivery_risk')\n",
    "    name_label=['Delivery Status_Advance shipping', 'Delivery Status_Late delivery','Delivery Status_Shipping canceled', 'Delivery Status_Shipping on time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OneHotEncoding:\n",
    "**Question 1:** <br>**Select categorical variables and transform them into numerical variables using OneHotEncoding (OHE).**\n",
    "\n",
    "**Hint**: \n",
    "- Select the categorical variables by dtype (object) or by number of unique values (categorical variables have a small number of unique values). <br>\n",
    "- You can go this [page](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) for more info on how to use One Hot Encoding or go to `Data_Science_crash_course.ipynb` in the Pre-bootcamp folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0lEUko-z6u4i",
    "outputId": "fac7e6e0-f37e-46da-e1a3-3d8d177df6bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Type', 'Delivery Status', 'Customer Country', 'Customer Segment', 'Market', 'Order Status', 'Shipping Mode']\n",
      "['Category Name', 'Customer City', 'Customer State', 'Department Name', 'Order City', 'Order Country', 'order date (DateOrders)', 'Order Region', 'Order State', 'Product Image', 'Product Name', 'shipping date (DateOrders)']\n"
     ]
    }
   ],
   "source": [
    "# If you need help selecting variables to onehotencode, you can run this following code. \n",
    "df_num=dataset.select_dtypes(exclude=np.dtype('O'))\n",
    "df_alphabetic=dataset.select_dtypes(include=np.dtype('O'))\n",
    "\n",
    "nb_unique_value_max=10\n",
    "to_OHE=[key for key in df_alphabetic.keys() if len(df_alphabetic[key].drop_duplicates())<nb_unique_value_max]\n",
    "to_label_encoder=[key for key in df_alphabetic.keys() if key not in to_OHE]\n",
    "\n",
    "print(to_OHE)\n",
    "print(to_label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(sparse_output=False)\n",
    "enc.fit(dataset[to_label_encoder])\n",
    "\n",
    "dataset_ohe = pd.DataFrame(enc.transform(dataset[to_label_encoder]))\n",
    "#dataset_clean_ohe = pd.concat([dataset_ohe], axis=1)\n",
    "dataset_ohe.head()\n",
    "#dataset_clean_ohe.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**: <br>\n",
    "**Concatenate the numerical variables that you've transformed with One Hot Encoding than output the dataframe.<br>**\n",
    "\n",
    "<u> Hint </u>: You can use pandas' `pd.concat()` function to concatenate dataframes together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use other data preprocessing methods if you want to after this question (Labelencoding, Normalization, Feature selection,...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this next cell, we have split the dataset into features (X_dataset) and target variable (y_dataset)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dataset=dataset.drop(name_label,axis=1) \n",
    "y_dataset=dataset[name_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XhryaZlk7c7R"
   },
   "source": [
    "\n",
    "**Question 3**: <br>\n",
    "**Now, split X and y into training and validation sets. <br>**\n",
    "\n",
    "<u> Hint </u>: You can use scikit-learn's `train_test_split` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zg8ZDc7E5LD3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9P9cvpH73he"
   },
   "source": [
    "# Fit your model\n",
    "\n",
    "Training Logistic, DecisionTree and Random forest models, using fit (you can see the differents parameters on scikit-learn website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "id": "v3DAahos6fY7",
    "outputId": "b3f1fcbc-e9bd-44a4-b3a3-23c05ab1485e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mJ2L5EbYHf31",
    "outputId": "6dc1c8ad-f54b-429f-f5cb-4df8eb25f92a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KCsfgUVXH6dU",
    "outputId": "ce008f48-bd21-47f2-afcd-1ac58f522237"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Fao6APIKCQv"
   },
   "source": [
    "It's more and less every time the same code ! Let's make a pipeline :\n",
    "\n",
    "Goal : create a function that takes an initialized untrained model as an argument and returns the score\n",
    "\n",
    "Train at least three other models with this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gE3MNRFVra2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fyI9tAtjezAa",
    "outputId": "ebea8cbb-078f-4c30-ac6e-230fe9404c88"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPA8yhL-Vr0h"
   },
   "source": [
    "Display the ROC curve for different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "zYDOE8Dvdl2I",
    "outputId": "e7805434-fc8e-453b-ef18-2fa2179ed827"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQuofktDkbwM"
   },
   "source": [
    "Create the confusion matrix function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPpKWbkQPRwS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    },
    "id": "arkp_xaxkvGv",
    "outputId": "18d359c9-6227-4237-c6c8-9d15478e04ff"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytj5AGeWI13g"
   },
   "source": [
    "# Prediction\n",
    "\n",
    "use your trained models to predict test labels (dataset in the hfactory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IvQKsuGJKOZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytj5AGeWI13g"
   },
   "source": [
    "\n",
    "\n",
    "# Upgrade your model !\n",
    "\n",
    "Use Grid Search, Cross validation and/or Random Search.\n",
    "\n",
    "Be careful, this part can be very heavy in terms of computing resources. Efficient coding! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
